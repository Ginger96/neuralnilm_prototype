

------- RESULTS ---------


WORKS (kind of):

82: 3 appliances, nice graphs.

3 appliances
124
126
138
139 quantization!
142 
145
148
157a not great
158 not bad

180 mse

5 appliances
129: gets the fridge (once)!
cross entropy, scale appliances

131c: probably the best for all 5 appliances
cross entropy, scale appliances, first 2 layers are dense layers

132a: about as good as 131c
cross entropy, scale appliances, first 2 layers are dense layers

132c: not as good as 131c

134a: not great but kind of works

137a: not great but kind of works
similar to 131c, but lots of pretraining

156: not great

357:
went back to doing regression using FF net.

Integration works (with no noise!): 413 

Integration works (kind of) with 3 appliances: 418.  Needs more
training?


424: attempt to get a net to remember a sequence of 256 numbers didn't
work very well at all!

########## FF AUTO ENCODER FOR SINGLE APPLIANCE

425: FF auto encoder with single appliance (Fridge)
RESULTS: works pretty well. Not perfectly.

426: add hair straighteners and TV.  Didn't learn well.  Maybe because
learning rate was too high to start???

427: Kind of worked.  Didn't do a great job of denoising.

428: Conv1D layer at bottom.  Worked well once I stopped saturating
the first dense layer!

429: 2 conv layers.  Down to a 32-dimensional vector in middle.  Works
pretty well!

430: 2 conv layers.  Down to a 16-dimensional vector in middle.
Appears to work as well as 32D vector.

431: try 4D vector in middle (!)  Works just as well!!!!

432: 5 appliances. 20% chance of skipping target appliance.  Still
comes down to 4D vector in middle ;)  Incredible results! Almost
certainly the best results I've had with all 5 appliances. Main issue
is a few false positives.

433: Exact same network as 432. But washer is target (not fridge).
It works really really well!!!

434: Same as 433 but with batch norm after every layer.

435: Same as *432* but with different houses for validation
and training.  Works pretty well.  But might be overfitting.

436: same as 435 but move house 4 from train to validation, and double
SEQ_LENGTH to 1024, and bring learning rate down faster.

437: Add dropout to 436.  Can't get it to run.  SHOULD TRY AGAIN.

438: 434 (dropout) with Source from 437 (i.e. houses 1,2,3 for train,
houses 4,5 for validation).  Try again!

439: 434 but longer seq and longer train.

440: 439 but 2048 seq and 32D middle layer
RESULTS: Hmm, didn't work well at all. Let's go back to
SEQ_LENGTH=1024

441: 440 but SEQ_LENGTH=1024

442: 441 but grid search over size of middle layer (1 to 64) and batch
norm / no batch norm
RESULTS: the larger the net, the better the performance HOWEVER, this
is unsurprising.  We need to compare validation costs on unseen houses
and data.

443: like 442m (32D middle, no batch norm, avg for 25 best = 0.131),
 experiment with conv or no conv
  a) remove 1 conv layer (0.140)
  b) remove both conv layers (0.156)
  c) 3 conv layers (0.131)
  d) 2 conv layers, double number of filters to 32 (0.134)
  e) 2 conv layers, 16 filters each, halve filter length to 2 (0.155)
  f) filter length = 3 (0.140)
  g) filter length = 8 (0.135)

444: just testing new dimshuffle layer

445: first test of DeConv1DLayer.  Just washing machine and hair straighteners.
  Works pretty well!
  Looks like we do want to reverse the last dimension of the weights,
  as per
  https://github.com/ChienliMa/DeConvNet/blob/master/DeConvNet/CPRStage.py#L118

446: DeConv1DLayer in a full AE

a: just 2 linear dense layers, tied weights, no conv
   avg valid cost =  0.0498397239 

b: just 2 linear conv layers, tied weights
   avg valid cost =  0.4122743905
   
c: 2 conv layers, tied weights, 1 rectify dense layer in between
   avg valid cost =  0.0315488130

d: b but with 4 filters
   avg valid cost =  0.4122795463
   
e: c but learning rate 1e-1 for all 50,000 iterations
   avg valid cost =  0.0151113402
   pretty stunning output. 20 is good example (with noise)
   Perhaps not perfect suppression of hair straighteners (see 27).  

f: 2 conv layers, tied weights, 1 dense linear layer in between
   avg valid cost =  0.0307678953
   
g: full AE with tied weights and 1 conv layer
   avg valid cost =  0.0442434102
   output looks pretty perfect ;) (apart from being a little 'blurry')

h: 4 linear convs, no dense layers, tied weights
   avg valid cost =  0.4093216062

i: 4 linear convs, no dense layers, untied weights
   avg valid cost =  0.4090858698

j: like g (full AE) but untied weights for conv layers
   avg valid cost =  0.0453109629

k: like g (full AE) but untied weights all through
   avg valid cost =  0.0438082032
   does better job than 'e' at suppressing hair straighteners (see 27)

l: like g (full AE) but untied weights for conv layers
   avg valid cost =  0.0452575982

m: like k but LR=1e-1 for all 50,000 epochs
   avg valid cost =  0.0318283364

n: like k but LR=1e-1 for all 50,000 epochs and only 
   3 middle dense layers (4084 x ReLU, 1021 x ReLU, 4084 x linear)
   Looks excellent. Pretty much as good reconstruction as 'e' but
   better suppression.
   avg valid cost =  0.0125398841

o: n but last dense layer is ReLU
   avg valid cost =  0.0108001959  <-- BEST YET!

p: n but with 4 conv layer. (set LR=0.01 manually)
   avg valid cost =  0.0290298369

q: n but only 128-d middle layer & rectify final dense layer
   I changed LR to 0.01 around 42000 iterations.
   avg valid cost =  0.0238524191

r: n but target is fridge & rectify final dense layer
   change LR=0.01 around 28000 iterations

t: n but target is TV & rectify final dense layer
   not bad but confuses TV and Fridge (e.g. see 10)
   IDEA: maybe try two conv layers at front to try to detect
   oscillations?  Or more dense layers?
   avg valid cost =  0.2374919057

v: t but 2 input conv layers (linear)
   avg valid cost =  0.2364626676

w: o but with ReLU conv layers
   avg valid cost =  0.2731662989

x: o but with ReLU for input conv, linear for output conv
   avg valid cost =  0.0118574286

y: o but with linear for input conv, ReLU for output conv
   avg valid cost =  0.2745352983

447: first attempt to disaggregate real mains power (using network
   learnt in 446o).  doesn't work too well.  Lots of false positives.
   Hence next thing to do will be to train on real data.

################### RandomSegments ########################

448: train on real data!

449: Reduce seq length to 512 and use RandomSegmentsInMemory

450: Independently standardise each input

451: ignore incomplete activations
  avg valid cost =  0.6222125888

452:
a: 451a but don't centre inputs individually
   avg valid cost =  0.5993861556

b: 451a but reduce seq length to 256
   avg valid cost =  0.3562454283
   Not an improvement, I don't think (possibly because
   sequences with complete activations are rarer)

c: 451a but reduced central layer to 4
   avg valid cost =  0.8279920220
   Bad. Appears to have just learnt an 'average' activation (with some
   shape to it)

d: 451a but reduced central layer to 8
   avg valid cost =  0.6588124633
   Not bad but not great.

e: 451a but reduced central layer to 32

f: 451a but reduce number of layers

g: 451a but no conv

############################# SameLocation ##############################

453: keep activations in same location (in time)

454:
a: like 453 but offset 50% of activations and ignore those.
avg valid cost =  0.0988993222

b: increase filters from 4 to 16
avg valid cost =  0.0859923661

c: no conv, 3 rectify layers, all SEQ_LENGTH in size
avg valid cost =  0.3136602640

d: 16d middle layer
avg valid cost =  0.1576996297

e: 64d middle layers
avg valid cost =  0.1260379851

f: no conv, 4 rectify layers
avg valid cost =  0.2055538595

455:
a: like 454b but change learning rate and longer seq and 8 filters
avg valid cost =  0.2308402061
Does a pretty decent job. false positives: 19 and 24

b: 4 filters
avg valid cost =  0.2294353545

c: 2 filters
avg valid cost =  0.2682644725

d: 8 filters and 2 additional layers
avg valid cost =  0.2024637163

e: like d but down to 64d middle layer
avg valid cost =  0.1990027875
Dose a nice job also. Still some false positives, esp 19 and 24

f-m: different appliances

f: coffee maker
doing a pretty stunning job.
TODO: Perhaps need to set min_on_duration and min_off_duration?  Although I'm not sure
the MSc students will have done that.

g: dish washer
some oddly aligned data, e.g. 21
TODO: set min_off_duration

h: hair dryer
TODO: set min_off_duration

i: kettle
really nice

j: oven
very nice

k: toaster
big false positives: 1, 11
TODO: min_off_duration and on_power_threshold (to ignore other things
plugged in)
very nice

l: light (kitchen lights)
also some odd alignment issues.  e.g. 6
not so good.  But it's quite a small signal

m: washer dryer
not doing a great job but training data is a bit duff:
TODO: need to set min_on_duration and min_off_duration

456: new code. Use different data for validation.  Better treatment of
min_off_duration and min_on_duration. SEQ_LENGTH = 1200.

457: fridge seq_length = 512. middle layer = 16d

458: don't offset targets. don't ignore any targets

459: stop before we overfit! (but do so in a rather naive way)

461: independently centre inputs

462: dropna for mains.  And ignore offset activations.

463: smaller dense0 and reduce num_filters to 4
Definitely reduces over fitting ;)

464: like 463 but with dropout after every dense layer.

465: exactly same as 463 except fixed bug where we'd only use the
first 64 activations for training!!!

466: back to large dense0 and |middle layer| = seq_length

467: adding 2 more dense layers, drop down to 32d in middle

468: train on houses 1,2,4. Validate on 5.
avg valid cost =  0.3297388554

469: same as 468 but |middle layer| = seq_length again
avg valid cost =  0.3469952643 

470: washer dryer. 4 filters. |middle layer| = 128
BUG: something weird with alignment of validation data.

471: same as 470 but only my house

472: same as 471 but high min_on_duration and min_off_duration for washer.

473: same as 472 but halve seq length

474: same as 473 but use houses 1,2,3,4 for train and 5 for validation
not working well.  washer in house 5 is quite odd!

475: train and validate on all 5 buildings
building 4's washing machine is odd (because NILMTK was taking min of
all appliances per meter)

476: train and validate on 1,2,3,5

477: 476 but take out additional two dense layers

478: divide n_seq_per_batch / len(validation_buildings) when throwing
away from data.  Longer seq.  Use house 4.

479: don't offset and ignore some activations. Also don't
independently center inputs.

----- some conclusions --------------
Using SameLocation works pretty well for appliances with an obvious
start (like fridges) but not so well for appliances without obvious
starts (like washing machines).  So will tinker with
RandomSegmentsInMemory.


################### RandomSegmentsInMemory ########################

480: RandomSegmentsInMemory, fridge, include incomplete activations

481: Increase learning rate to 1e-1. Independently centre inputs. And
use separate data for validation.

482: reduce SEQ_LENGTH from 800 to 512

################# PolygonOutputLayer #######################

483 : successfully use PolygonOutputLayer with 1 segment.

484, 485: attempting (and failing) to get PolygonOutputLayer to work
with multiple segments

############# Downsampled output ###########################

486: based on 482.  Subsample target 8x. Manually changed LR to 0.01

487: like 486 but ignore incomplete. And change learning rate to 0.01
Hmm, some problems with training examples.  There are some fridges
missing in the target!

488: (i think I've) fixed problem where some fridges were
absent. Ignore Freezer in house 4.  Also set on_power_threshold for
ignore chunks... hmm, no, some fridges are still missing!  Need to
analyse outputs some more.  I suspect I need to intersect mains good
sections with appliance good sections... or just don't drop NA and
throw away all target sequences with any NAs???

489: Another attempt to fix problem where some fridges not present.
best valid cost =  0.5164197683 at iteration  3080
avg valid cost =  0.5195092559

490: No final conv layer
Doesn't work quite as well as having conv layer at end

491: 489 but with washer dryer.

492: don't ignore incomplete

493: Use SameLocation. skip_probability=0.5.  offset_prob=1.

494: 493 but fixed bug where some seq without target might actually
have the target in them!

495: test on all appliances. ignore_incomplete=True,
allow_incomplete=True, include_all=True.
Some prelim thoughts: it does seem to prefer it if activations are
always towards the start of the sequence (perhaps not surprisingly).
a: avg valid cost =  0.2822513580

496: 495 but don't independently centre inputs
a: avg valid cost =  0.2852075994

497: independently centre inputs, don't offset, do include all.

i: 14 is good.  15 is good.

498: don't subsample target

499: shorter seq length for washing machine

################# RectangularSource #################

500: first attempt at training with RectangularSource.
     git: c04c1f7

501: reduce n_rectangular_segments to 3

502: reduce n_rectangular_segments to 2, plot cumsum.
     git: 4d191f6

503: don't plot cumsum!
     git: 80e0a2c

504: n_rectangular_segments back to 3

505: don't include_all.  And use format:'changepoints [0,1]'
     git: 219ef27

TODO:
* fix problem where it uses one segment for the spike.
* faster!

############### Start and end and mean #################
(a simplification of RectangularSource where we just output
the start point, end point and mean for each activation).

506: First attempt with target_is_start_and_end_and_mean=True
     git: e45626d

507: first attempt to plot power series with target.
* plot power series for target.  And turn start, end, mean into
  'proper' plot.

508: ignore_incomplete=True

509: try fixing bug where incomplete activations were still getting through.

510: more filters (4 -> 16).  Another dense layer.  higher learning rate.

511: independently centre inputs
     git: a243e20

512: try offset_probability = 0.9

513: * no conv layer

514: linear output (not standardised targets though), no conv layer
avg valid cost =  0.0200949628

515: linear output, 2 conv layers, lower learning rate
avg valid cost =  0.0193651617  (after about 400,000 iterations!)

516: linear output, 1 conv layer, high learning rate for a bit
a: no conv
avg valid cost =  0.0230021048

b: 1 conv layer
avg valid cost =  0.0197356101

c: 1 conv layer, filter size = 2
avg valid cost =  0.0202551596

d: 1 conv layer, ReLU
avg valid cost =  0.0210823435

e: conv then pool
avg valid cost =  0.0198636372

f: conv, pool, conv
avg valid cost =  0.0201312639

g: conv, conv
avg valid cost =  0.0191588160

h: conv, conv, conv
avg valid cost =  0.0198109504

i: conv, conv, conv, conv
avg valid cost =  0.0193489492

CONCLUSTIONS:
stick to linear Conv
no conv is worst
conv, conv is best

518:
like 517g.  But tweaked learning rate.  And train for ever.

519:
like 518 but don't independently center inputs.

######## RealApplianceSource with Recangular output #########
520: buggy
521: works but little training data
522: appears to work, and lots of training data.  Works pretty well!
avg valid cost =  0.0234354474

523: same as 522 but independently center inputs.  Works pretty well.
avg valid cost =  0.0233064983

524: same as 523 but increase skip probability for first appliance and
decrease it for others.  It's now 0.5 for all appliances.
avg valid cost =  0.0646887049

525: same as 524 but don't independently centre inputs
avg valid cost =  0.0612

526: change dense layers from 512, 256, 128 -> 512, 512, 512
avg train cost =  0.0373702273
avg valid cost =  0.0659154281
overfitting!

527: like 526 but add dropout layers
had to do lots of debugging to get this to work.  Basically invalid run.
oops, only 1 month of training data!

528: dropout, 0.5 skip prob for first appliance, 0.75 skip prob for
others.
oops, only 1 month of training data!

529: 528 but no dropout.
oops, only 1 month of training data!

530: skip layer from first conv
Not bad but not fantastic.  Doesn't seem to help much.

531: add another dense layer, skip from first to last dense layers.
Keep training rate at 1e-3.

532: exactly the same as 531 but validation now different to train!

533: same as 532 but only remove bare minimum of activations from
training set.

534: pad input before conv.  And allow 'distractor' appliances to be
anyway in the target seq.  No skip layers.  Much larger net.
Results: pretty good, actually!
First 272000 iterations: avg valid cost = 0.022723388
All iterations: 0.0203086548

535: same as 534 but smaller net.
Results: not quite as good as 534.
avg valid cost = 0.0248404965

536: 534 with:
 + batch norm on all dense layers
 + use nesterov_momentum (not clipped_nesterov_momentum)
 + reduce learning rate to 1e-4

537: 536 with:
 + batch norm on conv layers as well
 + reduce size of most dense layers by half

538: 537 but:
 + no batch norm on conv layers (because it doesn't respect conv
 property)
 + start with very low learning rate (to allow BN to burn in)

539: back to batch_norm with no kwargs.
BN is ruining training. Giving up on BN for now.

540: 535 but with dropout
not sure dropout is helping (although maybe it just needs to train for
longer?)

541: 540 but:
 + no dropout
 + multiple appliance outputs
 + don't use clipped_nesterov_momentum

542: same as 541 but prettier plotting, hopefully

TODO:
* try outputting a binary mask.  This might make skip connections more
effective.  And then, after training a binary mask, try adding a few
layers on top for regression?
* use diff
* different appliances
* Scale targets to [-1, 1]
* Mix RealApplianceSource with SamePosition.
* TanH output
* read Daniel Nouri's blog post on facial keypoint detection
* I'm a bit suspicious of some of the training data.  e.g. 513a_10
* pre-training (as AE?)
* two nets: one detects if there's a target anywhere in the input.
  The other says where the target is.
* instead of start, end, mean.  Try start, *width*, mean.
